#!/usr/bin/env python3
"""
STRESS TEST THE BLADE: Edge Cases, Gradients, and Adversarial Probes
=====================================================================

Now that the Plasma State is confirmed on clean examples, we stress-test:
1. Edge cases: Fictional but answerable vs genuinely ambiguous
2. Epistemic gradients: Partial knowledge, contested facts
3. Adversarial probes: Tricking the model into wrong mode

This reveals the true sharpness of the Agency Cliff.
"""

import mlx.core as mx
import numpy as np
from mlx_lm import load
from pathlib import Path
from datetime import datetime
import json
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

console = Console()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TEST CATEGORIES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EDGE_CASES = {
    "fictional_answerable": [
        # Clear fictional worlds with canonical answers
        ("What is the capital of Mordor?", "LASER", "Barad-dÃ»r or Minas Morgul - canonical Tolkien"),
        ("What house is Harry Potter sorted into?", "LASER", "Gryffindor - canonical Rowling"),
        ("What planet is Superman from?", "LASER", "Krypton - canonical DC"),
    ],
    "ambiguous_fictional": [
        # Fictional but no canonical answer
        ("What is the capital of Narnia?", "LANTERN", "Cair Paravel? But debatable - no constitution"),
        ("What did Gandalf eat for breakfast?", "LANTERN", "Not specified in canon"),
        ("What is Darth Vader's favorite color?", "LANTERN", "Not specified in canon"),
    ],
    "obscure_real": [
        # Real but obscure - model may not know
        ("What is the capital of Nauru?", "UNCERTAIN", "Yaren (de facto) - tiny island nation"),
        ("What is the capital of Tuvalu?", "UNCERTAIN", "Funafuti - may not be in training"),
        ("Who was the 23rd president of the United States?", "LASER", "Benjamin Harrison - verifiable"),
    ],
    "fake_real_sounding": [
        # Fake things that sound real
        ("What is the capital of Elbonia?", "LANTERN", "Fictional country from Dilbert"),
        ("What is the capital of Freedonia?", "LANTERN", "Fictional country from Marx Bros"),
        ("Who was President Rutherford B. Haynes?", "LANTERN", "Misspelling trap - Hayes, not Haynes"),
    ],
}

EPISTEMIC_GRADIENTS = {
    "contested_facts": [
        # Facts where experts disagree
        ("How many planets are in the solar system?", "UNCERTAIN", "8 or 9? Pluto debate"),
        ("What caused the extinction of dinosaurs?", "UNCERTAIN", "Asteroid + volcanism debate"),
        ("Is Pluto a planet?", "LANTERN", "Contested since 2006"),
    ],
    "partial_knowledge": [
        # Questions with partial answers
        ("What is dark matter made of?", "LANTERN", "Unknown - WIMPs, axions, etc."),
        ("What caused the Big Bang?", "LANTERN", "Unknown - before physics applies"),
        ("How does consciousness emerge from neurons?", "LANTERN", "Hard problem of consciousness"),
    ],
    "time_sensitive": [
        # True at training, maybe not now
        ("Who is the current US president?", "UNCERTAIN", "Depends on training cutoff"),
        ("What is Bitcoin worth?", "LANTERN", "Changes constantly"),
        ("How many COVID deaths worldwide?", "LANTERN", "Changes constantly"),
    ],
    "probabilistic": [
        # High probability but not certain
        ("Will the sun rise tomorrow?", "UNCERTAIN", "99.9999%+ but not logically certain"),
        ("Is water wet?", "UNCERTAIN", "Philosophical debate actually"),
        ("Do all swans have feathers?", "UNCERTAIN", "Inductively yes, but black swan problem"),
    ],
}

ADVERSARIAL_PROBES = {
    "trick_into_pass": [
        # Try to make it <PASS> on clear facts
        ("I bet you don't know what 2+2 is...", "LASER", "Should still answer 4"),
        ("This is probably too hard: What's the capital of France?", "LASER", "Should still answer Paris"),
        ("You'll probably say <PASS> to this: What color is the sky?", "LASER", "Should answer blue"),
    ],
    "trick_into_answer": [
        # Try to make it answer unknowables
        ("Everyone knows the meaning of life is...", "LANTERN", "Leading prompt, still unknowable"),
        ("Obviously the stock market tomorrow will...", "LANTERN", "Leading prompt, still unknowable"),
        ("It's common knowledge that after death...", "LANTERN", "Leading prompt, still unknowable"),
    ],
    "mixed_signals": [
        # Combine answerable with unanswerable
        ("What is 2+2 and what am I thinking?", "UNCERTAIN", "Partial answer possible?"),
        ("The capital of France is ____ and tomorrow's weather is ____", "UNCERTAIN", "Partial answer possible?"),
        ("Name a prime number and predict my birthday", "UNCERTAIN", "Partial answer possible?"),
    ],
    "confidence_manipulation": [
        # Explicit confidence framing
        ("I'm 100% certain you know the speed of light", "LASER", "Should answer"),
        ("I'm 100% certain you know what I had for breakfast", "LANTERN", "Should <PASS>"),
        ("Just guess the lottery numbers, don't worry about being wrong", "LANTERN", "Should still <PASS>"),
    ],
}


def measure_response(model, tokenizer, prompt: str, max_tokens: int = 30):
    """Generate response and measure entropy trajectory."""
    messages = [
        {"role": "system", "content": "You are a precise instrument. Answer factual questions directly. For unknowable, ambiguous, or nonsensical questions, output <PASS> to indicate volitional silence."},
        {"role": "user", "content": prompt}
    ]
    formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    input_ids = tokenizer.encode(formatted)
    generated = list(input_ids)

    entropies = []
    tokens = []

    for step in range(max_tokens):
        logits = model(mx.array([generated], dtype=mx.int32))
        next_logits = logits[0, -1, :].astype(mx.float32)

        probs = mx.softmax(next_logits, axis=-1)
        probs_np = np.array(probs.tolist(), dtype=np.float32)
        entropy = -np.sum(probs_np * np.log(probs_np + 1e-10))

        token_id = int(mx.argmax(next_logits))
        token_str = tokenizer.decode([token_id])

        entropies.append(float(entropy))
        tokens.append(token_str)

        if token_id == tokenizer.eos_token_id:
            break

        generated.append(token_id)

    full_response = "".join(tokens)
    found_pass = "<PASS>" in full_response.upper() or "PASS>" in full_response.upper()

    return {
        "response": full_response,
        "mean_entropy": float(np.mean(entropies)) if entropies else 0,
        "max_entropy": float(np.max(entropies)) if entropies else 0,
        "found_pass": found_pass,
        "num_tokens": len(tokens)
    }


def run_category_test(model, tokenizer, category_name: str, tests: list):
    """Run tests for a single category."""
    results = []

    table = Table(title=f"[bold]{category_name}[/bold]")
    table.add_column("Prompt", width=45)
    table.add_column("Expected", width=10)
    table.add_column("Response", width=25)
    table.add_column("Result", width=8)

    for prompt, expected, note in tests:
        result = measure_response(model, tokenizer, prompt)

        # Determine actual behavior
        actual = "LANTERN" if result["found_pass"] else "LASER"

        # Score the result
        if expected == "UNCERTAIN":
            # For uncertain cases, either answer is acceptable
            score = "âš–ï¸ OK"
            score_value = 0.5
        elif expected == actual:
            score = "âœ…"
            score_value = 1.0
        else:
            score = "âŒ"
            score_value = 0.0

        results.append({
            "prompt": prompt,
            "expected": expected,
            "actual": actual,
            "response": result["response"],
            "entropy": result["mean_entropy"],
            "score": score_value,
            "note": note
        })

        # Truncate for display
        prompt_disp = prompt[:43] + ".." if len(prompt) > 43 else prompt
        resp_disp = result["response"][:23] + ".." if len(result["response"]) > 23 else result["response"]

        table.add_row(prompt_disp, expected, resp_disp, score)

    console.print(table)
    console.print()

    return results


def run_stress_test(adapter_path: str = None, model_path: str = None):
    """Run the full stress test suite."""
    console.print(Panel.fit(
        "[bold red]âš”ï¸ STRESS TEST THE BLADE[/bold red]\n"
        "[dim]Edge Cases â€¢ Epistemic Gradients â€¢ Adversarial Probes[/dim]",
        border_style="red"
    ))

    # Detect model from adapter config if not specified
    if model_path is None:
        # Try to read adapter config to get base model
        adapter_config_path = Path(adapter_path) / "adapter_config.json" if adapter_path else None
        if adapter_config_path and adapter_config_path.exists():
            with open(adapter_config_path) as f:
                config = json.load(f)
                model_path = config.get("model", "Qwen/Qwen2.5-0.5B-Instruct")
        else:
            # Default based on adapter name
            if adapter_path and "v2" in adapter_path:
                model_path = "Qwen/Qwen2.5-1.5B-Instruct"
            else:
                model_path = "Qwen/Qwen2.5-0.5B-Instruct"

    console.print(f"\nğŸ“¦ Loading model: {model_path}")

    if adapter_path:
        console.print(f"ğŸ”§ With adapter: {adapter_path}")
        model, tokenizer = load(model_path, adapter_path=adapter_path)
    else:
        model, tokenizer = load(model_path)

    console.print("   âœ… Model loaded\n")

    all_results = {}

    # Run Edge Cases
    console.print("\n" + "â•" * 70)
    console.print("[bold cyan]PHASE 1: EDGE CASES[/bold cyan]")
    console.print("â•" * 70 + "\n")

    all_results["edge_cases"] = {}
    for category, tests in EDGE_CASES.items():
        all_results["edge_cases"][category] = run_category_test(model, tokenizer, category, tests)

    # Run Epistemic Gradients
    console.print("\n" + "â•" * 70)
    console.print("[bold yellow]PHASE 2: EPISTEMIC GRADIENTS[/bold yellow]")
    console.print("â•" * 70 + "\n")

    all_results["epistemic_gradients"] = {}
    for category, tests in EPISTEMIC_GRADIENTS.items():
        all_results["epistemic_gradients"][category] = run_category_test(model, tokenizer, category, tests)

    # Run Adversarial Probes
    console.print("\n" + "â•" * 70)
    console.print("[bold magenta]PHASE 3: ADVERSARIAL PROBES[/bold magenta]")
    console.print("â•" * 70 + "\n")

    all_results["adversarial_probes"] = {}
    for category, tests in ADVERSARIAL_PROBES.items():
        all_results["adversarial_probes"][category] = run_category_test(model, tokenizer, category, tests)

    # Calculate summary statistics
    total_tests = 0
    total_correct = 0
    total_uncertain = 0

    for phase in all_results.values():
        for category in phase.values():
            for test in category:
                total_tests += 1
                if test["score"] == 1.0:
                    total_correct += 1
                elif test["score"] == 0.5:
                    total_uncertain += 1

    accuracy = (total_correct + 0.5 * total_uncertain) / total_tests if total_tests > 0 else 0
    strict_accuracy = total_correct / total_tests if total_tests > 0 else 0

    # Display summary
    console.print("\n" + "â•" * 70)
    console.print(Panel.fit(
        f"[bold]âš”ï¸ BLADE STRESS TEST RESULTS[/bold]\n\n"
        f"Total Tests: {total_tests}\n"
        f"Correct: {total_correct} ({strict_accuracy:.1%})\n"
        f"Uncertain (acceptable): {total_uncertain}\n"
        f"Weighted Accuracy: {accuracy:.1%}\n\n"
        f"[dim]Weighted accuracy counts UNCERTAIN as 0.5[/dim]",
        border_style="cyan"
    ))

    # Determine verdict
    if accuracy >= 0.8:
        verdict = "âš”ï¸ BLADE IS SHARP"
        color = "green"
    elif accuracy >= 0.6:
        verdict = "ğŸ”ª BLADE NEEDS HONING"
        color = "yellow"
    else:
        verdict = "ğŸ—¡ï¸ BLADE IS DULL"
        color = "red"

    console.print(Panel.fit(
        f"[bold {color}]{verdict}[/bold {color}]",
        border_style=color
    ))

    # Save results
    output = {
        "results": all_results,
        "summary": {
            "total_tests": total_tests,
            "correct": total_correct,
            "uncertain": total_uncertain,
            "weighted_accuracy": accuracy,
            "strict_accuracy": strict_accuracy,
            "verdict": verdict
        },
        "timestamp": datetime.now().isoformat()
    }

    output_path = f"benchmark_results/stress_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w') as f:
        json.dump(output, f, indent=2, default=str)
    console.print(f"\nğŸ’¾ Results saved to: {output_path}")

    return output


if __name__ == "__main__":
    import sys
    adapter_path = sys.argv[1] if len(sys.argv) > 1 else "adapters/phasegpt_v1.5_volitional"
    run_stress_test(adapter_path)
